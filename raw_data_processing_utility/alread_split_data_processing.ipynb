{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage\n",
    "import tqdm\n",
    "import skimage.morphology\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "import tqdm\n",
    "import io\n",
    "\n",
    "from ipywidgets import interact, interact_manual\n",
    "from scipy.optimize import basinhopping\n",
    "from scipy import optimize\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from IPython.display import clear_output, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/data/Datasets/usg-kaggle/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bouncing_coordinate(coordinate_value, limit):\n",
    "    if coordinate_value >= 0: \n",
    "        if (coordinate_value // limit) % 2 == 0:\n",
    "            return coordinate_value % limit\n",
    "        return limit - (coordinate_value % limit) - 1\n",
    "    else:\n",
    "        if (abs(coordinate_value) // limit) % 2 == 0:\n",
    "            return limit - (coordinate_value % limit) -1\n",
    "        return coordinate_value % limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_values(center, angle, image):\n",
    "    x_c, y_c = center\n",
    "    a = np.tan(np.deg2rad(angle))\n",
    "    b = y_c - (a * x_c)\n",
    "    mask = np.zeros_like(image)\n",
    "    x1 = min(max(-b / (a + 1e-8), 0), image.shape[1])\n",
    "    y1 = int(a * x1 + b)\n",
    "    x1 = int(x1)\n",
    "    \n",
    "    start_point = np.asarray([[y1, x1]])\n",
    "    \n",
    "    x2 = min(max((image.shape[1] - b) / (a + 1e-8), 0), image.shape[1])\n",
    "    y2 = int(a * x2 + b)\n",
    "    x2 = int(x2)\n",
    "    \n",
    "    mask = np.zeros_like(image)\n",
    "    cv2.line(mask, (x1, y1), (x2, y2), 1, 1, cv2.LINE_AA)\n",
    "    points = np.stack(np.where(mask == 1), axis=-1)\n",
    "    values = []\n",
    "    kernel_size = 3\n",
    "    for y, x in points:\n",
    "        a_sum = 0\n",
    "        for of_y in range(-kernel_size // 2, kernel_size // 2 + 1):\n",
    "            for of_x in range(-kernel_size // 2, kernel_size // 2 + 1):\n",
    "                a_sum += image[\n",
    "                    get_bouncing_coordinate(y + of_y, image.shape[0]), \n",
    "                    get_bouncing_coordinate(x + of_x, image.shape[1])\n",
    "                ]\n",
    "        values.append(a_sum)\n",
    "    \n",
    "    distances = np.sqrt(np.sum((points - start_point) ** 2, axis=1))\n",
    "    return values, distances, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [path for path in data_path.glob('*') if path.is_dir()]\n",
    "dir_widget = widgets.Dropdown(\n",
    "    options=directories,\n",
    "    index=0,\n",
    "    description=\"Directory:\"\n",
    ")\n",
    "\n",
    "files = list([path for path in Path(dir_widget.value).rglob('lower.png') if not path.name.startswith('.')])\n",
    "files_widget = widgets.Dropdown(\n",
    "    options=files,\n",
    "    index=0,\n",
    "    description=\"File:\"\n",
    ")\n",
    "\n",
    "\n",
    "with files_widget.hold_trait_notifications():\n",
    "    files = list([path for path in Path(dir_widget.value).rglob('lower.png') if not path.name.startswith('.')])\n",
    "    files = set(files)\n",
    "    if len(files.difference(set(files_widget.options))) > 0:\n",
    "        files_widget.options = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_widget = widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=359,\n",
    "    value=90,\n",
    "    description=\"Angle:\"\n",
    ")\n",
    "\n",
    "\n",
    "@interact(a_dir=dir_widget, a_file=files_widget, angle=angle_widget)\n",
    "def get_img(a_dir, a_file, angle):\n",
    "    with files_widget.hold_trait_notifications():\n",
    "        files = list([path for path in Path(dir_widget.value).rglob('lower.png') if not path.name.startswith('.')])\n",
    "        files = set(files)\n",
    "        if len(files.difference(set(files_widget.options))) > 0:\n",
    "            files_widget.options = files\n",
    "        \n",
    "    img_file = cv2.imread(a_file.as_posix(), cv2.IMREAD_GRAYSCALE)\n",
    "    img_file = ((img_file.astype('float32') / 255) ** 8)\n",
    "    img_file = ((img_file - img_file.min()) / (img_file.max() - img_file.min()) * 255).astype('uint8')\n",
    "    coords = json.loads((a_file.parent / \"coordinates.json\").read_text())\n",
    "    circle_coords = coords[\"circle\"]\n",
    "    \n",
    "    mask = np.zeros_like(img_file)\n",
    "    radia = []\n",
    "    sums = []\n",
    "    circle_center = circle_coords[\"x\"], circle_coords[\"y\"]\n",
    "    for i in range(5, 50):\n",
    "        cv2.circle(mask, circle_center, i, 255, 1)\n",
    "        sums.append(img_file[mask == 255].sum() / (2 * np.pi * i))\n",
    "        radia.append(i)\n",
    "        mask[:] = 0\n",
    "        \n",
    "    gradients = np.diff(sums)\n",
    "    radia = radia[:-1]\n",
    "    ascendence = radia[np.argmax(gradients)]\n",
    "    descendence = radia[np.argmin(gradients)]\n",
    "    \n",
    "    orig = img_file.copy()\n",
    "    img_file = cv2.medianBlur(img_file, 3)\n",
    "    values, points, mask = get_pixel_values(circle_center, angle, img_file)\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 4))\n",
    "    ax[0].plot(radia, gradients)\n",
    "    ax[1].plot(points, values)\n",
    "    ax[2].plot(points[1:], np.diff(values))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18.6 * 2, 18.6), sharey=True)\n",
    "    ax[0].imshow(orig, cmap='gray')\n",
    "    ax[1].imshow(mask, cmap=\"gray\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    le_image = np.abs(orig.astype(np.float32) - img_file.astype(np.float32))\n",
    "    le_image = (le_image - le_image.min()) / (le_image.max() - le_image.min())\n",
    "    le_image[le_image > np.percentile(le_image, 98)] = 1\n",
    "    le_image[le_image <= np.percentile(le_image, 98)] = 0\n",
    "    \n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    le_image = cv2.medianBlur(le_image, 3)\n",
    "    \n",
    "    plt.figure(figsize=(18.6, 18.6))\n",
    "    plt.imshow(le_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = widgets.IntSlider(0, 0, 1049)\n",
    "y = widgets.IntSlider(0, 0, 325)\n",
    "width = widgets.IntSlider(1, 1, 1050)\n",
    "height = widgets.IntSlider(1, 1, 326)\n",
    "\n",
    "@interact(a_dir=dir_widget, a_file=files_widget, x=x, y=y, width=width, height=height)\n",
    "def estimate_noise(a_dir, a_file, x, y, width, height):\n",
    "    width = min(width + x, 1050) - x\n",
    "    height = min(height + y, 326) - y\n",
    "    img_file = cv2.imread(a_file.as_posix(), 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "    ax[0].imshow(img_file)\n",
    "    ax[1].imshow(img_file[y:y+height, x:x+width])\n",
    "    \n",
    "    sample = img_file[y:y+height, x:x+width]\n",
    "    print(sample.mean(), sample.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "def fit_img(img: np.ndarray) -> np.ndarray:\n",
    "    def get_min_max_cord(img, axis):\n",
    "        img = img.astype(np.float32)\n",
    "        line = img.sum(axis=axis)\n",
    "        line[line > 0] = 1\n",
    "        grad = np.diff(line)\n",
    "        a_min = np.argmax(grad)\n",
    "        a_max = np.argmin(grad)\n",
    "        return a_min, a_max\n",
    "    \n",
    "    y_min, y_max = get_min_max_cord(img, 1)\n",
    "    x_min, x_max = get_min_max_cord(img, 0)\n",
    "    return img[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "\n",
    "def low_pass_filter(img_block: np.ndarray, a: int = 8) -> np.ndarray:\n",
    "    dct_block = cv2.dct(img_block)\n",
    "    block_size = dct_block.shape[0]\n",
    "    assert 0 < a < block_size - 1, \"Parameter 'a' should be 0 < a < block_size \" \\\n",
    "        \"for block size equal to {}, received {} instead\".format(block_size, a)\n",
    "    \n",
    "    u, v = np.meshgrid(np.arange(0, block_size), np.arange(0, block_size))\n",
    "    u_minus_v = u - v\n",
    "    where_less_than_a = u_minus_v < a\n",
    "    dct_block[~where_less_than_a] = 0\n",
    "\n",
    "    restored = cv2.dct(dct_block, cv2.DCT_INVERSE)\n",
    "    \n",
    "    return restored\n",
    "\n",
    "def gaussian(img_block: np.ndarray, **kwargs) -> np.ndarray:\n",
    "    return cv2.blur(img_block, (7, 7))\n",
    "    \n",
    "def get_biggest_component(img):\n",
    "    binary = (img > 0).astype(np.uint8) * 255\n",
    "    connectivity = 8\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(binary, connectivity, cv2.CV_16U)\n",
    "    areas = stats[:, cv2.CC_STAT_AREA]\n",
    "    label_with_largest_area = np.argmax(areas)\n",
    "    indices = labels == np.argsort(-areas)[1]\n",
    "    img[~indices] = 0\n",
    "    return img\n",
    "    \n",
    "def get_clean_and_mask_images(img):\n",
    "    img = get_biggest_component(img)\n",
    "    img = fit_img(img)\n",
    "    \n",
    "    img_min, img_max = img.min(), img.max()\n",
    "    fl_img = img.astype(np.float32)\n",
    "    \n",
    "    hor_grad = fl_img[1:] - fl_img[:-1] \n",
    "    ver_grad = fl_img[:, 1:] - fl_img[:, :-1]\n",
    "    \n",
    "    hor_grad = hor_grad[:, 1:]\n",
    "    ver_grad = ver_grad[1:]\n",
    "    \n",
    "    grad = hor_grad + ver_grad\n",
    "    grad = (grad - grad.min()) / (grad.max() - grad.min())\n",
    "    grad = grad - grad.mean()\n",
    "    grad = np.clip(grad, 0, 1) \n",
    "    \n",
    "    offset_h = 50\n",
    "    offset_img = img[offset_h:]\n",
    "    percentile = np.percentile(grad, q=95)    \n",
    "    masked = (grad > percentile).astype(np.uint8)\n",
    "        \n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    masked = cv2.morphologyEx(masked, cv2.MORPH_DILATE, kernel, iterations=1)\n",
    "    masked = np.pad(masked, [[1, 0], [1, 0]], constant_values=0, mode=\"constant\")\n",
    "\n",
    "    \n",
    "    connectivity = 4\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(masked, connectivity, cv2.CV_16U)\n",
    "    areas = stats[:, cv2.CC_STAT_AREA]\n",
    "    for i, area in enumerate(areas):\n",
    "        if area < 256:\n",
    "            indices = labels == i\n",
    "            masked[indices] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    removed = img * (1 - masked)\n",
    "    output_img = np.zeros_like(img).astype(np.float32)\n",
    "    \n",
    "    block_size = 64\n",
    "    step = block_size // 2\n",
    "    output_img = cv2.inpaint(img, masked, 5, cv2.INPAINT_TELEA)\n",
    "    return output_img, masked\n",
    "    \n",
    "\n",
    "@interact(a_dir=dir_widget, \n",
    "          a_file=files_widget)\n",
    "def wut(a_dir, a_file):\n",
    "    print(a_file)\n",
    "    img = cv2.imread(a_file.as_posix(), 0)\n",
    "    output_img, masked = get_clean_and_mask_images(img)\n",
    " \n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1, figsize=(32, 32))\n",
    "    ax[0].imshow(masked, cmap='gray')\n",
    "    ax[1].imshow(output_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, init_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_u = self.n_d = [16, 16, 16, 16 ,16]\n",
    "        self.k_u = self.k_d = [3, 3, 3, 3, 3]\n",
    "        self.n_s = [4, 4, 4, 4, 4]\n",
    "        self.k_s = [1, 1, 1, 1, 1]\n",
    "        self.init_channels = init_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.coding_layers = nn.ModuleList()\n",
    "        self.decoding_layers = nn.ModuleList()\n",
    "        self.skip_layers = nn.ModuleList()\n",
    "        \n",
    "        next_filters = self.init_channels\n",
    "        decoder_input_filters = []\n",
    "        \n",
    "        for i in range(len(self.n_u)):\n",
    "            self.coding_layers.append(\n",
    "                self._encoder_block(next_filters, self.n_u[i], self.k_u[i])\n",
    "            )\n",
    "            self.skip_layers.append(\n",
    "                self._skip_block(self.n_u[i], self.n_s[i], self.k_s[i])\n",
    "            )\n",
    "            \n",
    "            next_filters = self.n_u[i]\n",
    "            decoder_input_filters.append(\n",
    "                self.n_u[i] + self.n_s[i]\n",
    "            )\n",
    "            \n",
    "        decoder_input_filters = decoder_input_filters[::-1]\n",
    "        for i in range(len(self.n_d)):\n",
    "            self.decoding_layers.append(\n",
    "                self._decoder_block(decoder_input_filters[i], self.n_d[i], self.k_d[i])\n",
    "            )\n",
    "            \n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Conv2d(self.n_d[-1], self.out_channels, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_results = []\n",
    "        for i, layer in enumerate(self.coding_layers):\n",
    "            x = layer(x)\n",
    "            skip_results.append(\n",
    "                self.skip_layers[i](x)\n",
    "            )\n",
    "        \n",
    "        skip_results = skip_results[::-1]\n",
    "        \n",
    "        for i, layer in enumerate(self.decoding_layers):\n",
    "            without_skip = x\n",
    "            x = torch.cat((x, skip_results[i]), dim=1)\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.out_layer(x)\n",
    "        \n",
    "        \n",
    "    def _encoder_block(self, in_channels, filters, kernel_size, dilation=1):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters, kernel_size, 2, kernel_size // 2, bias=False, padding_mode=\"reflection\", dilation=dilation),\n",
    "            nn.InstanceNorm2d(filters),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, kernel_size, 1, kernel_size // 2, bias=False, padding_mode=\"reflection\", dilation=dilation),\n",
    "            nn.InstanceNorm2d(filters),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        return layers\n",
    "    \n",
    "    def _decoder_block(self, in_channels, filters, kernel_size, dilation=1):\n",
    "        layers = nn.Sequential(\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, filters, kernel_size, 1, kernel_size // 2, bias=False, padding_mode=\"reflection\", dilation=dilation),\n",
    "            nn.InstanceNorm2d(filters),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(filters, filters, 1, 1, 0, bias=False, dilation=dilation),\n",
    "            nn.InstanceNorm2d(filters),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        )\n",
    "        return layers\n",
    "    \n",
    "    def _skip_block(self, in_channels, filters, kernel_size):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters, kernel_size, 1, 0, bias=False),\n",
    "            nn.InstanceNorm2d(filters),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(y_pred, y_true, mask):\n",
    "    diff = (y_pred - y_true) * mask\n",
    "    loss = diff.pow(2).mean()\n",
    "    return loss\n",
    "\n",
    "def match_dims_to_be_divisible(an_img):\n",
    "    h, w = an_img.shape[0], an_img.shape[1]\n",
    "    \n",
    "    new_h = (h // 32 + 1) * 32\n",
    "    new_w = (w // 32 + 1) * 32\n",
    "    return new_h, new_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model = Unet(128, 1)\n",
    "\n",
    "an_img = cv2.imread(\"/data/Datasets/usg-kaggle/train/1/497/lower.png\", 0)\n",
    "orig_img = fit_img(get_biggest_component(an_img))\n",
    "an_img, mask = get_clean_and_mask_images(ori)\n",
    "orig_h, orig_w = an_img.shape\n",
    "\n",
    "new_h, new_w = match_dims_to_be_divisible(an_img)\n",
    "an_img = cv2.resize(an_img, (new_w, new_h))\n",
    "mask = 1 - cv2.resize(mask, (new_w, new_h))\n",
    "orig_img = cv2.resize(orig_img, (new_w, new_h))\n",
    "\n",
    "an_img = an_img.astype(np.float32) / 255\n",
    "mask = mask.astype(np.float32)\n",
    "an_img = an_img * mask\n",
    "\n",
    "h, w = an_img.shape\n",
    "\n",
    "an_img = torch.from_numpy(an_img).float().unsqueeze(0).unsqueeze(0)\n",
    "mask = torch.from_numpy(mask).float().unsqueeze(0).unsqueeze(0)\n",
    "optimised_image = torch.randn((1, 128, h, w)).float()\n",
    "optimised_image.uniform_(0, 1)\n",
    "optimised_image.requires_grad = True\n",
    "\n",
    "num_iters = 2000\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "optimiser = optim.Adam(a_model.parameters(), lr=lr)\n",
    "out_widget = widgets.Output()\n",
    "display(out_widget)\n",
    "print()\n",
    "for i in range(num_iters):\n",
    "    with out_widget:\n",
    "        print(\"\\rIter: {} / {}\".format(i + 1, num_iters), end='')\n",
    "    optimiser.zero_grad()\n",
    "    prediction = a_model(optimised_image)\n",
    "    loss = reconstruction_loss(prediction, an_img, mask)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        pred_img = prediction[0, 0].detach().cpu().numpy() * 255\n",
    "        pred_img = np.clip(pred_img, 0, 255).astype(np.uint8)\n",
    "        pred_img = np.concatenate([pred_img, orig_img], axis=1)\n",
    "        pil_iamge = PILImage.fromarray(pred_img)\n",
    "        img_byte_array = io.BytesIO()\n",
    "        pil_iamge.save(img_byte_array, format=\"PNG\")\n",
    "        img_byte_array = img_byte_array.getvalue()\n",
    "        print()\n",
    "        clear_output()\n",
    "        display(out_widget)\n",
    "        display(Image(img_byte_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "an_img = cv2.imread(\"/data/Datasets/usg-kaggle/train/1/156/lower.png\", 0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(an_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
